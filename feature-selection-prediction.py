# -*- coding: utf-8 -*-
"""Machine-Learning_Assignment-1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d_r4jHhLhSWLKTe-8EagZX_P77kPLbDc

*BY: ISMAIL ABU SAIID*

# Evaluation of Feature Selection & K-Fold Cross Validation on Car Pricing Prediction

**Importing Libraries**
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense

"""**Loading the Data**"""

# Loading the dataset into a pandas dataframe
data = pd.read_csv('car_details.csv')

"""**Data Preprocessing**"""

# Dropping the ID and name columns
data = data.drop(['ID', 'name'], axis=1)

# Printing the first 5 columns
print(data.head())

print(data.info())

# Encoding the categorical variables using one-hot encoding
encoded_data = pd.get_dummies(data)

print(encoded_data.info())

"""**Model Creation & Training**

Without Feature Selection
"""

import numpy as np
from sklearn.model_selection import KFold

# Split the data into features and target
X = encoded_data.drop('price', axis=1)
y = encoded_data['price']

# Define the neural network architecture
model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(X.shape[1],)))
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(1))

# Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Define the number of folds for cross-validation
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# Train and evaluate the model using cross-validation
mse_scores = []
mae_scores = []
for train_index, test_index in kfold.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    model.fit(X_train, y_train, epochs=100) #, callbacks=EarlyStopping(patience=3))

    mse, mae = model.evaluate(X_test, y_test)
    mse_scores.append(mse)
    mae_scores.append(mae)

# Print the mean and standard deviation of the cross-validation scores
print(f'Mean squared error: {sum(mse_scores)/len(mse_scores):.2f}')
print(f'Mean absolute error: {sum(mae_scores)/len(mae_scores):.2f}')

"""**Feature Selection**"""

import seaborn as sns
import matplotlib.pyplot as plt

#Finding correlated columns for Feature selection
corr = data.corr()
fig, ax = plt.subplots(figsize=(15,15))
sns.heatmap(corr,annot=True)

corr2 = encoded_data.corr()
fig, ax = plt.subplots(figsize=(50,50))
sns.heatmap(corr2,annot=True)

#Dropping fully correlated columns
selected_data = encoded_data.drop(["fuelsystem_idi", "compressionratio", "highwaympg", "cylindernumber_two", "enginetype_rotor"], axis = 1)

# Split the selected features
X_train, X_test, y_train, y_test = train_test_split(selected_data.drop('price', axis=1), selected_data['price'], test_size=0.2, random_state=42)

# Scale the data: Scale the numerical features to a similar range.
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train.select_dtypes(include='number'))
X_test_scaled = scaler.transform(X_test.select_dtypes(include='number'))

# Select features: Use SelectKBest from the scikit-learn library to select the top 30 features based on the f_regression method.

from sklearn.feature_selection import SelectKBest, f_regression

selector = SelectKBest(f_regression, k=30)
X_train_selected = selector.fit_transform(X_train_scaled, y_train)
X_test_selected = selector.transform(X_test_scaled)

selected_columns = X_train.columns[selector.get_support()]
print(f'Selected columns: {selected_columns}')

selected_data = selected_data.loc[:, ['price','wheelbase', 'carlength', 'carwidth', 'curbweight', 'enginesize',
       'boreratio', 'horsepower', 'citympg', 'aspiration_std',
       'aspiration_turbo', 'carbody_convertible', 'carbody_hardtop',
       'carbody_hatchback', 'drivewheels_fwd', 'drivewheels_rwd',
       'enginelocation_front', 'enginelocation_rear', 'enginetype_dohc',
       'enginetype_dohcv', 'enginetype_l', 'enginetype_ohc', 'enginetype_ohcv',
       'cylindernumber_eight', 'cylindernumber_five', 'cylindernumber_four',
       'cylindernumber_six', 'cylindernumber_twelve', 'fuelsystem_1bbl',
       'fuelsystem_2bbl', 'fuelsystem_mpfi']]

print(selected_data.info())

"""**Model Creation & Training**

With Feature Selection
"""

#Training and Testing the model with the new selected features

# Split the data into features and target
X = selected_data.iloc[:, 1:]
y = selected_data['price']

# Define the neural network architecture
model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(X.shape[1],)))
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(1))

# Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Define the number of folds for cross-validation
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# Train and evaluate the model using cross-validation
mse_scores_FeatureSelection = []
mae_scores_FeatureSelection = []
for train_index, test_index in kfold.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    model.fit(X_train, y_train, epochs=100)

    mse, mae = model.evaluate(X_test, y_test)
    mse_scores_FeatureSelection.append(mse)
    mae_scores_FeatureSelection.append(mae)

# Print the mean and standard deviation of the cross-validation scores to be compared with the first MSE and MAE containing all Features
print(f'Mean squared error: {sum(mse_scores_FeatureSelection)/len(mse_scores_FeatureSelection):.2f}')
print(f'Mean absolute error: {sum(mae_scores_FeatureSelection)/len(mae_scores_FeatureSelection):.2f}')

"""**Performance Analysis**"""

# Making Graphs to compare results

import matplotlib.pyplot as plt

# Create a list of folds for the x-axis
folds = [i+1 for i in range(len(mae_scores))]

# Plot the MAE values for both sets on the same graph
plt.scatter(folds, mae_scores, color="g", marker= 'o', label='Original')
plt.scatter(folds, mae_scores_FeatureSelection, color="c", marker= '*', label='With Feature Selection')

# Add labels and title
plt.xlabel('Folds')
plt.ylabel('MAE')
plt.title('Mean Absolute Error Comparison')

# Add legend
plt.legend()

# Show the plot
plt.show()

print(data['price'])

#Comparing Price with MAE average price of the first 5 Cars

price_list = data.loc[:4, 'price'].tolist()
pricing = [i+1 for i in range(len(price_list))]

MAE1_upper_price = [x + (sum(mae_scores)/len(mae_scores)) for x in price_list]
MAE1_lower_price = [x - (sum(mae_scores)/len(mae_scores)) for x in price_list]

MAE2_upper_price = [x + (sum(mae_scores_FeatureSelection)/len(mae_scores_FeatureSelection)) for x in price_list]
MAE2_lower_price = [x - (sum(mae_scores_FeatureSelection)/len(mae_scores_FeatureSelection)) for x in price_list]



# Plot the MAE values for both sets on the graph with the price of cars
plt.plot(pricing, MAE1_upper_price, color="g", marker="^", label='Original_MAE_UPPER' )
plt.plot(pricing, price_list, color="r", marker="o", label='Car Prices')
plt.plot(pricing, MAE1_lower_price, color="g", marker="v", label='Original_MAE_LOWER')

# Add labels and title
plt.xlabel('Cars')
plt.ylabel('Pricing')
plt.title('Original MAE & Actual Price Comparison')

# Add legend
plt.legend()

# Show the plot
plt.show()


plt.plot(pricing, MAE2_upper_price, color="c", marker="^", label='Feature Selection_MAE_UPPER')
plt.plot(pricing, price_list, color="r", marker="o", label='Car Prices')
plt.plot(pricing, MAE2_lower_price, color="c", marker="v", label='Feature Selection_MAE_LOWER')

# Add labels and title
plt.xlabel('Cars')
plt.ylabel('Pricing')
plt.title('Selected Features MAE & Actual Price Comparison')

# Add legend
plt.legend(loc='lower center')

# Show the plot
plt.show()